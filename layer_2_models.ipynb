{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Julian Domingo - jad5348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "# Data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling stuff\n",
    "from xgboost import XGBClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                              AdaBoostClassifier,\n",
    "                              ExtraTreesClassifier,\n",
    "                              BaggingClassifier)\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Computation / numerical\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlens.preprocessing import Subset\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv(\"./data/raw/test.csv\")[[\"id\"]]\n",
    "y_train = pd.read_csv(\"./data/raw/train.csv\")[\"Y\"].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_preds(preds, preds_filename):\n",
    "    submission = pd.DataFrame({\"id\": test_ids.id, \"Y\": preds})\n",
    "    submission.to_csv(\"./submissions/stacking_{}.csv\".format(preds_filename), index=False, columns=[\"id\", \"Y\"])\n",
    "    \n",
    "    \n",
    "def get_data(filename):\n",
    "    train = pd.read_csv(\"./data/refined/train/train_{}.csv\".format(filename))\n",
    "    test = pd.read_csv(\"./data/refined/test/test_{}.csv\".format(filename))\n",
    "    \n",
    "    x_train = train.drop([\"Y\"], axis = 1)\n",
    "    y_train = train[\"Y\"]\n",
    "    \n",
    "    return train, test, x_train, y_train\n",
    "\n",
    "\n",
    "def get_cross_val_score(model, x_train, y_train, n_folds, run_parallel=True):\n",
    "    if run_parallel:\n",
    "        cv = cross_val_score(model, x_train, y_train, cv = n_folds, scoring = \"roc_auc\", n_jobs = -1)\n",
    "    else:\n",
    "        cv = cross_val_score(model, x_train, y_train, cv = n_folds, scoring = \"roc_auc\")\n",
    "\n",
    "    print(\"Cross validation score: {} +/- {}\\nRaw scores: {}\".format(str(np.mean(cv)), str(np.std(cv)), str(cv)))\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_raw, test_raw, x_train_raw, y_train_raw = get_data(\"raw\")\n",
    "train_base, test_base, x_train_base, y_train_base = get_data(\"base\")\n",
    "train_log, test_log, x_train_log, y_train_log = get_data(\"log\")\n",
    "train_poly, test_poly, x_train_poly, y_train_poly = get_data(\"poly\")\n",
    "train_scaled, test_scaled, x_train_scaled, y_train_scaled = get_data(\"scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Class\n",
    "\n",
    "Stacking works well for small to medium-sized data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, base_learners, meta_learners, y_train, test_ids):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learners = meta_learners\n",
    "        self.y_train = y_train\n",
    "        self.test_ids = test_ids\n",
    "    \n",
    "    \n",
    "    def get_indiv_meta_preds(self, meta):\n",
    "        \"\"\" Retrieves the predictions of the model 'meta'. \"\"\"\n",
    "        if self.meta_features_train is None or self.meta_features_test is None:\n",
    "            raise ValueError(\"Invoke 'get_meta_features' before predicting.\")\n",
    "            \n",
    "        meta.fit(self.meta_features_train, self.y_train)\n",
    "        meta_preds = meta.predict_proba(self.meta_features_test)[:,1]\n",
    "        return meta_preds\n",
    "        \n",
    "        \n",
    "    def get_meta_features(self):\n",
    "        \"\"\" Retrieves all meta features for the train & test data from the base learners specified. \"\"\"\n",
    "        self.meta_features_train = np.zeros((len(self.y_train), len(self.base_learners)))\n",
    "        self.meta_features_test = np.zeros((len(self.test_ids), len(self.base_learners)))\n",
    "        \n",
    "        for i, base in enumerate(self.base_learners):\n",
    "            print (\"Gathering meta feature from '{}'...\".format(base))\n",
    "            self.meta_features_train[:, i] = pd.read_csv(\"./meta_features/train/train_{}.csv\".format(base), \\\n",
    "                                                         index_col=0).as_matrix().ravel()\n",
    "            self.meta_features_test[:, i] = pd.read_csv(\"./meta_features/test/test_{}.csv\".format(base), \\\n",
    "                                                        index_col=0).as_matrix().ravel()\n",
    "            \n",
    "        return self.meta_features_train.copy(), self.meta_features_test.copy()\n",
    "    \n",
    "    \n",
    "    def fit_meta_learners_and_predict(self):\n",
    "        \"\"\" Generates predictions using all meta features generated from the base learners for each meta learner. \"\"\"\n",
    "        if not self.meta_features_train or self.meta_features_test:\n",
    "            raise ValueError(\"get_meta_features() should be called before generate_out_of_folds_preds.\")\n",
    "        \n",
    "        self.meta_learner_preds = np.zeros(len(self.test_ids), len(self.meta_learners))\n",
    "        \n",
    "        for i, meta in enumerate(meta_learners):\n",
    "            meta.fit(self.meta_features_train, self.y_train)\n",
    "            self.meta_learner_preds[:, i] =  meta.predict_proba(self.meta_features_test)[:,1]\n",
    "            \n",
    "    \n",
    "    def get_df_meta_learner_preds(self):\n",
    "        if self.meta_learner_preds is None:\n",
    "            raise ValueError(\"No predictions were found. Invoke 'fit_meta_learners_and_predict' first.\")\n",
    "        \n",
    "        if self.base_learners is None:\n",
    "            raise ValueError(\"No base learners were specified. Construct an instance with base learners.\")\n",
    "        \n",
    "        return pd.DataFrame(self.meta_learner_preds, columns=self.base_learners)\n",
    "    \n",
    "    \n",
    "    def get_final_preds(self):\n",
    "        return np.mean(self.meta_learner_preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learner(s) Parameter Tuning & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble 1\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "   \n",
    "**Meta Learners: **\n",
    "    * RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_learners_v1 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\"\n",
    "]\n",
    "\n",
    "rf_meta_v1 = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "meta_learners_v1 = [rf_meta_v1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n"
     ]
    }
   ],
   "source": [
    "stacker_v1 = Stacker(base_learners_v1, meta_learners_v1, y_train, test_ids)\n",
    "meta_features_train_v1, meta_features_test_v1 = stacker_v1.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [5, 6, 7], 'n_estimators': [300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: this block takes a long time to execute.\n",
    "rf_meta_v1_param_grid = {\n",
    "    \"max_features\": range(5, 7 + 1),\n",
    "    \"max_depth\": range(5, 10 + 1),\n",
    "    \"n_estimators\": range(300, 1000 + 1, 100) \n",
    "}\n",
    "\n",
    "gs_rf_v1 = GridSearchCV(estimator=rf_meta_v1, param_grid=rf_meta_v1_param_grid, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1)\n",
    "gs_rf_v1.fit(meta_features_train_v1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 6, 'n_estimators': 300, 'max_depth': 6}\n",
      "0.772539082175\n"
     ]
    }
   ],
   "source": [
    "print gs_rf_v1.best_params_\n",
    "print gs_rf_v1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.771359868469 +/- 0.00723717612684\n",
      "Raw scores: [ 0.76715711  0.77956341  0.75947714  0.77377431  0.77682737]\n",
      "[ 0.76715711  0.77956341  0.75947714  0.77377431  0.77682737]\n"
     ]
    }
   ],
   "source": [
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=6, max_features=6), meta_features_train_v1, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97136003  0.67447269  0.94252603 ...,  0.88289829  0.98178302\n",
      "  0.96819782]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/ tuned params\n",
    "meta_preds_v1 = stacker_v1.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features=6, max_depth=6))\n",
    "print meta_preds_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.7  0.9 ...,  1.   1.   1. ]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/o tuned params\n",
    "meta_preds_v1_untuned = stacker_v1.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1))\n",
    "print meta_preds_v1_untuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the predictions without tuned parameters, tuning of the meta model makes a tremendous difference on the prediction set obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensemble 2\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "    * KNN_{2, 4, 8, 16, 32, 64, 128, 256, 512, 1024}\n",
    "   \n",
    "**Meta Learners: **\n",
    "    * RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'knn_2'...\n",
      "Gathering meta feature from 'knn_4'...\n",
      "Gathering meta feature from 'knn_8'...\n",
      "Gathering meta feature from 'knn_16'...\n",
      "Gathering meta feature from 'knn_32'...\n",
      "Gathering meta feature from 'knn_64'...\n",
      "Gathering meta feature from 'knn_128'...\n",
      "Gathering meta feature from 'knn_256'...\n",
      "Gathering meta feature from 'knn_512'...\n",
      "Gathering meta feature from 'knn_1024'...\n"
     ]
    }
   ],
   "source": [
    "base_learners_v2 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"knn_2\",\n",
    "    \"knn_4\",\n",
    "    \"knn_8\",\n",
    "    \"knn_16\",\n",
    "    \"knn_32\",\n",
    "    \"knn_64\",\n",
    "    \"knn_128\",\n",
    "    \"knn_256\",\n",
    "    \"knn_512\",\n",
    "    \"knn_1024\"\n",
    "]\n",
    "\n",
    "rf_meta_v2 = RandomForestClassifier(n_jobs=-1)\n",
    "meta_learners_v2 = [rf_meta_v2]\n",
    "\n",
    "stacker_v2 = Stacker(base_learners_v2, meta_learners_v2, y_train, test_ids)\n",
    "meta_features_train_v2, meta_features_test_v2 = stacker_v2.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [5, 6, 7], 'n_estimators': [300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: this block takes a long time to execute.\n",
    "rf_meta_v2_param_grid = {\n",
    "    \"max_features\": range(5, 7 + 1),\n",
    "    \"max_depth\": range(5, 10 + 1),\n",
    "    \"n_estimators\": range(300, 1000 + 1, 100) \n",
    "}\n",
    "\n",
    "gs_rf_v2 = GridSearchCV(estimator=rf_meta_v2, param_grid=rf_meta_v2_param_grid, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1)\n",
    "gs_rf_v2.fit(meta_features_train_v2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 5, 'n_estimators': 300, 'max_depth': 9}\n",
      "0.773074767924\n"
     ]
    }
   ],
   "source": [
    "print gs_rf_v2.best_params_\n",
    "print gs_rf_v2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.769897637451 +/- 0.00448583181217\n",
      "Raw scores: [ 0.76837689  0.77614972  0.77087385  0.76246286  0.77162487]\n",
      "[ 0.76837689  0.77614972  0.77087385  0.76246286  0.77162487]\n"
     ]
    }
   ],
   "source": [
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5), meta_features_train_v2, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97460188  0.66699757  0.95640802 ...,  0.91079086  0.98644606\n",
      "  0.97122791]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/ tuned params\n",
    "meta_preds_v2_tuned = stacker_v2.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5))\n",
    "print meta_preds_v2_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_preds(meta_preds_v2_tuned, \"_\".join(base_learners_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble 3\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "    * Bagged XGB (50 runs)\n",
    "    * Extra Trees Base\n",
    "    \n",
    "**Meta Learners: **\n",
    "    * RF\n",
    "    * XGB\n",
    "    \n",
    "**Final Predictions**: harmonic mean of meta learner predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_bag'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'extra_trees_base'...\n"
     ]
    }
   ],
   "source": [
    "base_learners_v3 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\"\n",
    "]\n",
    "\n",
    "rf_meta_v3 = RandomForestClassifier(n_jobs=-1)\n",
    "meta_learners_v3 = [rf_meta_v3]\n",
    "\n",
    "stacker_v3 = Stacker(base_learners_v3, meta_learners_v3, y_train, test_ids)\n",
    "meta_features_train_v3, meta_features_test_v3 = stacker_v3.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_learners = [RandomForestClassifier(), XGBClassifier()]\n",
    "meta_learners = [RandomForestClassifier()]\n",
    "\n",
    "v3_param_grid = {\n",
    "#     \"xgb\":\n",
    "#     {\n",
    "#         \"learning_rate\": uniform(0.01, 0.2),\n",
    "#         \"n_estimators\": randint(300, 600),\n",
    "#         \"max_depth\": randint(3, 10),\n",
    "#         \"min_child_weight\": uniform(0.5, 1.5),\n",
    "#         \"gamma\": uniform(0, 0.2),\n",
    "#         \"subsample\": uniform(0.5, 0.9),\n",
    "#         \"colsample_bytree\": uniform(0.5, 0.9),\n",
    "#     },\n",
    "    \"rf\":\n",
    "    {\n",
    "        \"max_features\": randint(3, 7),\n",
    "        \"max_depth\": randint(5, 10),\n",
    "        \"n_estimators\": randint(300, 600)\n",
    "    }\n",
    "}\n",
    "\n",
    "base_learners_v3_models = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed),\n",
    "    BaggingClassifier(base_estimator=XGBClassifier(learning_rate=0.1,\n",
    "                                                    n_estimators=345,\n",
    "                                                    max_depth=8,\n",
    "                                                    min_child_weight=2,\n",
    "                                                    gamma=0.2,\n",
    "                                                    subsample=0.6,\n",
    "                                                    colsample_bytree=0.5,\n",
    "                                                    objective=\"binary:logistic\",\n",
    "                                                    n_jobs=-1,\n",
    "                                                    scale_pos_weight=1,\n",
    "                                                    seed=seed), \n",
    "                      n_estimators=50, max_samples=0.7, max_features=0.75, bootstrap_features=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Running an entire ensemble several times just to compare different meta learners can be prohibitvely expensive. ML-Ensemble implements a class that acts as a transformer, allowing you to use ingoing layers as a \"preprocessing\" step, so that you need to only evaluate the meta learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the layers you don't want to tune into an ensemble with model selection turned on\n",
    "in_layer = SuperLearner(model_selection=True)\n",
    "in_layer.add(base_learners_v3_models)\n",
    "\n",
    "preprocess = [in_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda2/lib/python2.7/site-packages/mlens/model_selection/model_selection.py:600: UserWarning: No valid parameters found for meta.randomforestclassifier. Will fit and score once with given parameter settings.\n",
      "  \"settings.\".format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job\n",
      "Preprocessing 1 preprocessing pipelines over 5 CV folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda2/lib/python2.7/site-packages/xgboost-0.7-py2.7.egg/xgboost/sklearn.py:200: DeprecationWarning: The seed parameter is deprecated as of version .6.Please use random_state instead.seed is deprecated.\n",
      "  'seed is deprecated.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "evl = Evaluator(\n",
    "    make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True),\n",
    "    cv=n_splits,\n",
    "    random_state=seed,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "evl.fit(\n",
    "    meta_features_train_v3, \n",
    "    y_train,\n",
    "    meta_learners,\n",
    "    v3_param_grid,\n",
    "    preprocessing={\"meta\": preprocess},\n",
    "    n_iter=4                            # bump this up to do a larger grid search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(evl.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-ensemble Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_meta_mlens_v1 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=6)\n",
    "\n",
    "mlens_base_learners = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed)\n",
    "]\n",
    "\n",
    "ensemble_v1 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v1.add(mlens_base_learners, proba=True)\n",
    "ensemble_v1.add_meta(rf_meta_mlens_v1, proba=True)\n",
    "mlens_v1_preds = ensemble_v1.fit(meta_features_train_v1, y_train).predict_proba(meta_features_test_v1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9640277   0.70394427  0.95102447 ...,  0.89943802  0.98440754\n",
      "  0.95957553]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v1_preds\n",
    "save_preds(mlens_v1_preds, \"mlens_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_meta_mlens_v2 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5)\n",
    "\n",
    "mlens_base_learners = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=2),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=4),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=8),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=16),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=32),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=64),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=128),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=256),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=512),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=1024)\n",
    "]\n",
    "\n",
    "ensemble_v2 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v2.add(mlens_base_learners, proba=True)\n",
    "ensemble_v2.add_meta(rf_meta_mlens_v2, proba=True)\n",
    "mlens_v2_preds = ensemble_v2.fit(meta_features_train_v2, y_train).predict_proba(meta_features_test_v2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97149235  0.76364833  0.96501911 ...,  0.92276353  0.98632556\n",
      "  0.96678698]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v2_preds\n",
    "save_preds(mlens_v2_preds, \"mlens_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the LB scores, it seems adding KNeighborsClassifier models don't improve the stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_meta_mlens_v3 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5)\n",
    "\n",
    "mlens_base_learners_v3 = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed),\n",
    "    BaggingClassifier(base_estimator=XGBClassifier(learning_rate=0.1,\n",
    "                                                    n_estimators=345,\n",
    "                                                    max_depth=8,\n",
    "                                                    min_child_weight=2,\n",
    "                                                    gamma=0.2,\n",
    "                                                    subsample=0.6,\n",
    "                                                    colsample_bytree=0.5,\n",
    "                                                    objective=\"binary:logistic\",\n",
    "                                                    n_jobs=-1,\n",
    "                                                    scale_pos_weight=1,\n",
    "                                                    seed=seed), \n",
    "                      n_estimators=50, max_samples=0.7, max_features=0.75, bootstrap_features=True)\n",
    "]\n",
    "\n",
    "ensemble_v3 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v3.add(mlens_base_learners_v3, proba=True)\n",
    "ensemble_v3.add_meta(rf_meta_mlens_v3, proba=True)\n",
    "mlens_v3_preds = ensemble_v3.fit(meta_features_train_v3, y_train).predict_proba(meta_features_test_v3)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_forest_raw</th>\n",
       "      <th>random_forest_log</th>\n",
       "      <th>random_forest_poly</th>\n",
       "      <th>xgboost_raw</th>\n",
       "      <th>xgboost_base</th>\n",
       "      <th>xgboost_poly</th>\n",
       "      <th>xgboost_base</th>\n",
       "      <th>xgboost_bag</th>\n",
       "      <th>logistic_regression_log</th>\n",
       "      <th>adaboost_base</th>\n",
       "      <th>extra_trees_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.938816</td>\n",
       "      <td>0.894083</td>\n",
       "      <td>0.682318</td>\n",
       "      <td>0.894083</td>\n",
       "      <td>0.958313</td>\n",
       "      <td>0.912384</td>\n",
       "      <td>0.501568</td>\n",
       "      <td>0.908163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.997241</td>\n",
       "      <td>0.994538</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>0.994538</td>\n",
       "      <td>0.992380</td>\n",
       "      <td>0.935845</td>\n",
       "      <td>0.501776</td>\n",
       "      <td>0.953350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.994968</td>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.997939</td>\n",
       "      <td>0.991531</td>\n",
       "      <td>0.980984</td>\n",
       "      <td>0.950247</td>\n",
       "      <td>0.501754</td>\n",
       "      <td>0.939202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.999019</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.994178</td>\n",
       "      <td>0.966773</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.969778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.958</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.951995</td>\n",
       "      <td>0.964378</td>\n",
       "      <td>0.965681</td>\n",
       "      <td>0.964378</td>\n",
       "      <td>0.922798</td>\n",
       "      <td>0.937737</td>\n",
       "      <td>0.500819</td>\n",
       "      <td>0.928275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.883</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.960177</td>\n",
       "      <td>0.982531</td>\n",
       "      <td>0.960177</td>\n",
       "      <td>0.954669</td>\n",
       "      <td>0.937885</td>\n",
       "      <td>0.500930</td>\n",
       "      <td>0.948550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.905</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.926855</td>\n",
       "      <td>0.918181</td>\n",
       "      <td>0.970848</td>\n",
       "      <td>0.918181</td>\n",
       "      <td>0.942915</td>\n",
       "      <td>0.935959</td>\n",
       "      <td>0.502308</td>\n",
       "      <td>0.932186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.985542</td>\n",
       "      <td>0.992321</td>\n",
       "      <td>0.989989</td>\n",
       "      <td>0.992321</td>\n",
       "      <td>0.983988</td>\n",
       "      <td>0.945346</td>\n",
       "      <td>0.501293</td>\n",
       "      <td>0.937382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.979</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.996340</td>\n",
       "      <td>0.994799</td>\n",
       "      <td>0.993424</td>\n",
       "      <td>0.994799</td>\n",
       "      <td>0.964199</td>\n",
       "      <td>0.942417</td>\n",
       "      <td>0.501341</td>\n",
       "      <td>0.941501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.984</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.997901</td>\n",
       "      <td>0.995806</td>\n",
       "      <td>0.996160</td>\n",
       "      <td>0.995806</td>\n",
       "      <td>0.971569</td>\n",
       "      <td>0.938500</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>0.940962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.814</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.938595</td>\n",
       "      <td>0.806608</td>\n",
       "      <td>0.910593</td>\n",
       "      <td>0.806608</td>\n",
       "      <td>0.934898</td>\n",
       "      <td>0.933016</td>\n",
       "      <td>0.523464</td>\n",
       "      <td>0.945950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.988332</td>\n",
       "      <td>0.987530</td>\n",
       "      <td>0.989142</td>\n",
       "      <td>0.987530</td>\n",
       "      <td>0.980795</td>\n",
       "      <td>0.957145</td>\n",
       "      <td>0.500964</td>\n",
       "      <td>0.928622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.893</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.983279</td>\n",
       "      <td>0.997874</td>\n",
       "      <td>0.991655</td>\n",
       "      <td>0.997874</td>\n",
       "      <td>0.994795</td>\n",
       "      <td>0.967233</td>\n",
       "      <td>0.504610</td>\n",
       "      <td>0.968997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.961</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.995353</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>0.974989</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>0.988976</td>\n",
       "      <td>0.947085</td>\n",
       "      <td>0.501666</td>\n",
       "      <td>0.914179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.830810</td>\n",
       "      <td>0.918554</td>\n",
       "      <td>0.920837</td>\n",
       "      <td>0.918554</td>\n",
       "      <td>0.869024</td>\n",
       "      <td>0.949587</td>\n",
       "      <td>0.500919</td>\n",
       "      <td>0.938830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.998739</td>\n",
       "      <td>0.995228</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>0.995228</td>\n",
       "      <td>0.976185</td>\n",
       "      <td>0.942347</td>\n",
       "      <td>0.501912</td>\n",
       "      <td>0.957030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.933</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.988812</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>0.988812</td>\n",
       "      <td>0.980110</td>\n",
       "      <td>0.944435</td>\n",
       "      <td>0.501573</td>\n",
       "      <td>0.955404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.572</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.726632</td>\n",
       "      <td>0.640872</td>\n",
       "      <td>0.652339</td>\n",
       "      <td>0.640872</td>\n",
       "      <td>0.783822</td>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.500833</td>\n",
       "      <td>0.929719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.995394</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.986705</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.992181</td>\n",
       "      <td>0.930019</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.933854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.999440</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>0.996915</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>0.993228</td>\n",
       "      <td>0.943523</td>\n",
       "      <td>0.501147</td>\n",
       "      <td>0.954399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.952542</td>\n",
       "      <td>0.852864</td>\n",
       "      <td>0.952542</td>\n",
       "      <td>0.837431</td>\n",
       "      <td>0.953873</td>\n",
       "      <td>0.500640</td>\n",
       "      <td>0.912794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.851</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.981393</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.962556</td>\n",
       "      <td>0.957767</td>\n",
       "      <td>0.962278</td>\n",
       "      <td>0.935031</td>\n",
       "      <td>0.500633</td>\n",
       "      <td>0.881875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.903</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.990614</td>\n",
       "      <td>0.993048</td>\n",
       "      <td>0.993232</td>\n",
       "      <td>0.993048</td>\n",
       "      <td>0.961057</td>\n",
       "      <td>0.937322</td>\n",
       "      <td>0.501198</td>\n",
       "      <td>0.931966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.960</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.935041</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.924922</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.945781</td>\n",
       "      <td>0.501639</td>\n",
       "      <td>0.940470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.970721</td>\n",
       "      <td>0.925707</td>\n",
       "      <td>0.983214</td>\n",
       "      <td>0.925707</td>\n",
       "      <td>0.919647</td>\n",
       "      <td>0.917307</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.917530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.997155</td>\n",
       "      <td>0.998104</td>\n",
       "      <td>0.998501</td>\n",
       "      <td>0.998104</td>\n",
       "      <td>0.996322</td>\n",
       "      <td>0.954858</td>\n",
       "      <td>0.502817</td>\n",
       "      <td>0.959759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.986</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999473</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.995040</td>\n",
       "      <td>0.919254</td>\n",
       "      <td>0.502659</td>\n",
       "      <td>0.956082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.985554</td>\n",
       "      <td>0.972294</td>\n",
       "      <td>0.984455</td>\n",
       "      <td>0.972294</td>\n",
       "      <td>0.975696</td>\n",
       "      <td>0.941280</td>\n",
       "      <td>0.501712</td>\n",
       "      <td>0.936901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.999665</td>\n",
       "      <td>0.999696</td>\n",
       "      <td>0.999665</td>\n",
       "      <td>0.996543</td>\n",
       "      <td>0.944297</td>\n",
       "      <td>0.502455</td>\n",
       "      <td>0.959178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.794</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.883907</td>\n",
       "      <td>0.915632</td>\n",
       "      <td>0.955831</td>\n",
       "      <td>0.915632</td>\n",
       "      <td>0.808318</td>\n",
       "      <td>0.955413</td>\n",
       "      <td>0.501061</td>\n",
       "      <td>0.941061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16353</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.997493</td>\n",
       "      <td>0.994368</td>\n",
       "      <td>0.998239</td>\n",
       "      <td>0.994368</td>\n",
       "      <td>0.978078</td>\n",
       "      <td>0.972099</td>\n",
       "      <td>0.502810</td>\n",
       "      <td>0.940006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>0.985</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.997080</td>\n",
       "      <td>0.996005</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996005</td>\n",
       "      <td>0.988544</td>\n",
       "      <td>0.973813</td>\n",
       "      <td>0.524724</td>\n",
       "      <td>0.958083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998290</td>\n",
       "      <td>0.999037</td>\n",
       "      <td>0.999121</td>\n",
       "      <td>0.999037</td>\n",
       "      <td>0.996658</td>\n",
       "      <td>0.952112</td>\n",
       "      <td>0.502595</td>\n",
       "      <td>0.960338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16356</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.997256</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.998544</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.972486</td>\n",
       "      <td>0.940776</td>\n",
       "      <td>0.501310</td>\n",
       "      <td>0.940917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16357</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.997272</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>0.998529</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>0.985849</td>\n",
       "      <td>0.942296</td>\n",
       "      <td>0.501388</td>\n",
       "      <td>0.958088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>0.949</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.803074</td>\n",
       "      <td>0.958075</td>\n",
       "      <td>0.930910</td>\n",
       "      <td>0.958075</td>\n",
       "      <td>0.941873</td>\n",
       "      <td>0.954083</td>\n",
       "      <td>0.501190</td>\n",
       "      <td>0.952499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>0.959</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.987583</td>\n",
       "      <td>0.981676</td>\n",
       "      <td>0.980604</td>\n",
       "      <td>0.981676</td>\n",
       "      <td>0.973340</td>\n",
       "      <td>0.930311</td>\n",
       "      <td>0.500884</td>\n",
       "      <td>0.947086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>0.905</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.997390</td>\n",
       "      <td>0.998003</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.998003</td>\n",
       "      <td>0.993844</td>\n",
       "      <td>0.938621</td>\n",
       "      <td>0.501409</td>\n",
       "      <td>0.943213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.980257</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>0.980257</td>\n",
       "      <td>0.972102</td>\n",
       "      <td>0.937988</td>\n",
       "      <td>0.501146</td>\n",
       "      <td>0.931656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.933435</td>\n",
       "      <td>0.939385</td>\n",
       "      <td>0.965842</td>\n",
       "      <td>0.939385</td>\n",
       "      <td>0.948651</td>\n",
       "      <td>0.921131</td>\n",
       "      <td>0.500972</td>\n",
       "      <td>0.922317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.992866</td>\n",
       "      <td>0.998133</td>\n",
       "      <td>0.997866</td>\n",
       "      <td>0.998133</td>\n",
       "      <td>0.991696</td>\n",
       "      <td>0.922129</td>\n",
       "      <td>0.520682</td>\n",
       "      <td>0.977681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16364</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.997729</td>\n",
       "      <td>0.998747</td>\n",
       "      <td>0.998083</td>\n",
       "      <td>0.998747</td>\n",
       "      <td>0.976116</td>\n",
       "      <td>0.917509</td>\n",
       "      <td>0.501697</td>\n",
       "      <td>0.940016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16365</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.995050</td>\n",
       "      <td>0.995052</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.995052</td>\n",
       "      <td>0.971896</td>\n",
       "      <td>0.956198</td>\n",
       "      <td>0.502489</td>\n",
       "      <td>0.958775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16366</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.992435</td>\n",
       "      <td>0.994770</td>\n",
       "      <td>0.996648</td>\n",
       "      <td>0.994770</td>\n",
       "      <td>0.974605</td>\n",
       "      <td>0.930459</td>\n",
       "      <td>0.501824</td>\n",
       "      <td>0.932921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16367</th>\n",
       "      <td>0.985</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.989270</td>\n",
       "      <td>0.989585</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.989585</td>\n",
       "      <td>0.978716</td>\n",
       "      <td>0.953686</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.953966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16368</th>\n",
       "      <td>0.885</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.985135</td>\n",
       "      <td>0.949460</td>\n",
       "      <td>0.972716</td>\n",
       "      <td>0.949460</td>\n",
       "      <td>0.925359</td>\n",
       "      <td>0.959734</td>\n",
       "      <td>0.501151</td>\n",
       "      <td>0.933029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16369</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.949134</td>\n",
       "      <td>0.985797</td>\n",
       "      <td>0.949134</td>\n",
       "      <td>0.945565</td>\n",
       "      <td>0.962759</td>\n",
       "      <td>0.501341</td>\n",
       "      <td>0.937401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16370</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.941315</td>\n",
       "      <td>0.965544</td>\n",
       "      <td>0.971430</td>\n",
       "      <td>0.965544</td>\n",
       "      <td>0.965733</td>\n",
       "      <td>0.925530</td>\n",
       "      <td>0.501913</td>\n",
       "      <td>0.952027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16371</th>\n",
       "      <td>0.603</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.873856</td>\n",
       "      <td>0.891703</td>\n",
       "      <td>0.967788</td>\n",
       "      <td>0.891703</td>\n",
       "      <td>0.902892</td>\n",
       "      <td>0.954414</td>\n",
       "      <td>0.501333</td>\n",
       "      <td>0.814025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16372</th>\n",
       "      <td>0.979</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.973587</td>\n",
       "      <td>0.993105</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.993105</td>\n",
       "      <td>0.975707</td>\n",
       "      <td>0.949549</td>\n",
       "      <td>0.501102</td>\n",
       "      <td>0.932882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.767004</td>\n",
       "      <td>0.905827</td>\n",
       "      <td>0.854559</td>\n",
       "      <td>0.905827</td>\n",
       "      <td>0.861466</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>0.501076</td>\n",
       "      <td>0.679697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16374</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.995027</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>0.998222</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>0.989311</td>\n",
       "      <td>0.949872</td>\n",
       "      <td>0.502044</td>\n",
       "      <td>0.954603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>0.676</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.475540</td>\n",
       "      <td>0.474369</td>\n",
       "      <td>0.414622</td>\n",
       "      <td>0.474369</td>\n",
       "      <td>0.841802</td>\n",
       "      <td>0.956850</td>\n",
       "      <td>0.501307</td>\n",
       "      <td>0.955261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16376</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.992717</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.979316</td>\n",
       "      <td>0.941861</td>\n",
       "      <td>0.501457</td>\n",
       "      <td>0.955670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>0.984</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.981412</td>\n",
       "      <td>0.985117</td>\n",
       "      <td>0.981412</td>\n",
       "      <td>0.955683</td>\n",
       "      <td>0.940081</td>\n",
       "      <td>0.501314</td>\n",
       "      <td>0.931658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.995899</td>\n",
       "      <td>0.996878</td>\n",
       "      <td>0.995554</td>\n",
       "      <td>0.996878</td>\n",
       "      <td>0.992124</td>\n",
       "      <td>0.950846</td>\n",
       "      <td>0.502340</td>\n",
       "      <td>0.957395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16379</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.987516</td>\n",
       "      <td>0.979334</td>\n",
       "      <td>0.993302</td>\n",
       "      <td>0.979334</td>\n",
       "      <td>0.944108</td>\n",
       "      <td>0.924664</td>\n",
       "      <td>0.501063</td>\n",
       "      <td>0.928478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16380</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.987169</td>\n",
       "      <td>0.984491</td>\n",
       "      <td>0.989290</td>\n",
       "      <td>0.984491</td>\n",
       "      <td>0.951024</td>\n",
       "      <td>0.936418</td>\n",
       "      <td>0.501865</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16381</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.995962</td>\n",
       "      <td>0.995838</td>\n",
       "      <td>0.992313</td>\n",
       "      <td>0.995838</td>\n",
       "      <td>0.986485</td>\n",
       "      <td>0.959669</td>\n",
       "      <td>0.501615</td>\n",
       "      <td>0.955774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382</th>\n",
       "      <td>0.951</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.978558</td>\n",
       "      <td>0.981549</td>\n",
       "      <td>0.977603</td>\n",
       "      <td>0.981549</td>\n",
       "      <td>0.976710</td>\n",
       "      <td>0.960789</td>\n",
       "      <td>0.501182</td>\n",
       "      <td>0.959291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16383 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       random_forest_raw  random_forest_log  random_forest_poly  xgboost_raw  \\\n",
       "0                  0.871              0.902               0.831     0.938816   \n",
       "1                  0.971              0.973               0.977     0.997241   \n",
       "2                  0.982              0.982               0.976     0.994968   \n",
       "3                  0.978              0.989               0.957     0.999398   \n",
       "4                  0.958              0.933               0.941     0.951995   \n",
       "5                  0.883              0.898               0.888     0.973064   \n",
       "6                  0.905              0.922               0.896     0.926855   \n",
       "7                  0.996              0.996               0.986     0.985542   \n",
       "8                  0.979              0.972               0.987     0.996340   \n",
       "9                  0.984              0.985               0.981     0.997901   \n",
       "10                 0.814              0.790               0.790     0.938595   \n",
       "11                 0.971              0.975               0.969     0.988332   \n",
       "12                 0.893              0.897               0.863     0.983279   \n",
       "13                 0.961              0.953               0.964     0.995353   \n",
       "14                 0.941              0.895               0.935     0.830810   \n",
       "15                 0.994              0.982               0.989     0.998739   \n",
       "16                 0.933              0.938               0.913     0.993401   \n",
       "17                 0.572              0.613               0.623     0.726632   \n",
       "18                 0.978              0.979               0.971     0.995394   \n",
       "19                 0.994              0.989               0.991     0.999440   \n",
       "20                 0.776              0.796               0.774     0.930746   \n",
       "21                 0.851              0.832               0.824     0.981393   \n",
       "22                 0.903              0.912               0.909     0.990614   \n",
       "23                 0.960              0.947               0.957     0.935041   \n",
       "24                 0.866              0.791               0.860     0.970721   \n",
       "25                 0.991              0.994               0.989     0.997155   \n",
       "26                 0.991              0.986               1.000     0.999627   \n",
       "27                 0.988              0.986               0.996     0.985554   \n",
       "28                 0.988              0.982               0.984     0.999224   \n",
       "29                 0.794              0.762               0.873     0.883907   \n",
       "...                  ...                ...                 ...          ...   \n",
       "16353              0.977              0.977               0.983     0.997493   \n",
       "16354              0.985              0.984               0.984     0.997080   \n",
       "16355              0.996              0.990               0.998     0.998290   \n",
       "16356              0.997              0.998               0.992     0.997256   \n",
       "16357              0.983              0.984               0.983     0.997272   \n",
       "16358              0.949              0.955               0.947     0.803074   \n",
       "16359              0.959              0.928               0.937     0.987583   \n",
       "16360              0.905              0.922               0.912     0.997390   \n",
       "16361              0.938              0.961               0.961     0.979332   \n",
       "16362              0.932              0.925               0.929     0.933435   \n",
       "16363              0.989              0.989               0.989     0.992866   \n",
       "16364              0.988              0.985               0.977     0.997729   \n",
       "16365              0.991              0.980               0.973     0.995050   \n",
       "16366              0.890              0.931               0.897     0.992435   \n",
       "16367              0.985              0.985               0.986     0.989270   \n",
       "16368              0.885              0.901               0.858     0.985135   \n",
       "16369              0.924              0.892               0.906     0.934500   \n",
       "16370              0.922              0.930               0.919     0.941315   \n",
       "16371              0.603              0.585               0.654     0.873856   \n",
       "16372              0.979              0.974               0.974     0.973587   \n",
       "16373              0.763              0.711               0.818     0.767004   \n",
       "16374              0.964              0.961               0.952     0.995027   \n",
       "16375              0.676              0.655               0.703     0.475540   \n",
       "16376              0.994              0.992               0.988     0.994697   \n",
       "16377              0.984              0.977               0.988     0.941550   \n",
       "16378              0.978              0.978               0.968     0.995899   \n",
       "16379              0.991              0.997               0.989     0.987516   \n",
       "16380              0.978              0.968               0.971     0.987169   \n",
       "16381              0.987              0.992               0.983     0.995962   \n",
       "16382              0.951              0.950               0.922     0.978558   \n",
       "\n",
       "       xgboost_base  xgboost_poly  xgboost_base  xgboost_bag  \\\n",
       "0          0.894083      0.682318      0.894083     0.958313   \n",
       "1          0.994538      0.994200      0.994538     0.992380   \n",
       "2          0.991531      0.997939      0.991531     0.980984   \n",
       "3          0.998098      0.999019      0.998098     0.994178   \n",
       "4          0.964378      0.965681      0.964378     0.922798   \n",
       "5          0.960177      0.982531      0.960177     0.954669   \n",
       "6          0.918181      0.970848      0.918181     0.942915   \n",
       "7          0.992321      0.989989      0.992321     0.983988   \n",
       "8          0.994799      0.993424      0.994799     0.964199   \n",
       "9          0.995806      0.996160      0.995806     0.971569   \n",
       "10         0.806608      0.910593      0.806608     0.934898   \n",
       "11         0.987530      0.989142      0.987530     0.980795   \n",
       "12         0.997874      0.991655      0.997874     0.994795   \n",
       "13         0.988109      0.974989      0.988109     0.988976   \n",
       "14         0.918554      0.920837      0.918554     0.869024   \n",
       "15         0.995228      0.998789      0.995228     0.976185   \n",
       "16         0.988812      0.986500      0.988812     0.980110   \n",
       "17         0.640872      0.652339      0.640872     0.783822   \n",
       "18         0.993631      0.986705      0.993631     0.992181   \n",
       "19         0.998430      0.996915      0.998430     0.993228   \n",
       "20         0.952542      0.852864      0.952542     0.837431   \n",
       "21         0.957767      0.962556      0.957767     0.962278   \n",
       "22         0.993048      0.993232      0.993048     0.961057   \n",
       "23         0.929515      0.924922      0.929515     0.950462   \n",
       "24         0.925707      0.983214      0.925707     0.919647   \n",
       "25         0.998104      0.998501      0.998104     0.996322   \n",
       "26         0.999512      0.999473      0.999512     0.995040   \n",
       "27         0.972294      0.984455      0.972294     0.975696   \n",
       "28         0.999665      0.999696      0.999665     0.996543   \n",
       "29         0.915632      0.955831      0.915632     0.808318   \n",
       "...             ...           ...           ...          ...   \n",
       "16353      0.994368      0.998239      0.994368     0.978078   \n",
       "16354      0.996005      0.996933      0.996005     0.988544   \n",
       "16355      0.999037      0.999121      0.999037     0.996658   \n",
       "16356      0.997347      0.998544      0.997347     0.972486   \n",
       "16357      0.998336      0.998529      0.998336     0.985849   \n",
       "16358      0.958075      0.930910      0.958075     0.941873   \n",
       "16359      0.981676      0.980604      0.981676     0.973340   \n",
       "16360      0.998003      0.996906      0.998003     0.993844   \n",
       "16361      0.980257      0.967246      0.980257     0.972102   \n",
       "16362      0.939385      0.965842      0.939385     0.948651   \n",
       "16363      0.998133      0.997866      0.998133     0.991696   \n",
       "16364      0.998747      0.998083      0.998747     0.976116   \n",
       "16365      0.995052      0.997002      0.995052     0.971896   \n",
       "16366      0.994770      0.996648      0.994770     0.974605   \n",
       "16367      0.989585      0.995671      0.989585     0.978716   \n",
       "16368      0.949460      0.972716      0.949460     0.925359   \n",
       "16369      0.949134      0.985797      0.949134     0.945565   \n",
       "16370      0.965544      0.971430      0.965544     0.965733   \n",
       "16371      0.891703      0.967788      0.891703     0.902892   \n",
       "16372      0.993105      0.980856      0.993105     0.975707   \n",
       "16373      0.905827      0.854559      0.905827     0.861466   \n",
       "16374      0.990882      0.998222      0.990882     0.989311   \n",
       "16375      0.474369      0.414622      0.474369     0.841802   \n",
       "16376      0.993333      0.992717      0.993333     0.979316   \n",
       "16377      0.981412      0.985117      0.981412     0.955683   \n",
       "16378      0.996878      0.995554      0.996878     0.992124   \n",
       "16379      0.979334      0.993302      0.979334     0.944108   \n",
       "16380      0.984491      0.989290      0.984491     0.951024   \n",
       "16381      0.995838      0.992313      0.995838     0.986485   \n",
       "16382      0.981549      0.977603      0.981549     0.976710   \n",
       "\n",
       "       logistic_regression_log  adaboost_base  extra_trees_base  \n",
       "0                     0.912384       0.501568          0.908163  \n",
       "1                     0.935845       0.501776          0.953350  \n",
       "2                     0.950247       0.501754          0.939202  \n",
       "3                     0.966773       0.502810          0.969778  \n",
       "4                     0.937737       0.500819          0.928275  \n",
       "5                     0.937885       0.500930          0.948550  \n",
       "6                     0.935959       0.502308          0.932186  \n",
       "7                     0.945346       0.501293          0.937382  \n",
       "8                     0.942417       0.501341          0.941501  \n",
       "9                     0.938500       0.503214          0.940962  \n",
       "10                    0.933016       0.523464          0.945950  \n",
       "11                    0.957145       0.500964          0.928622  \n",
       "12                    0.967233       0.504610          0.968997  \n",
       "13                    0.947085       0.501666          0.914179  \n",
       "14                    0.949587       0.500919          0.938830  \n",
       "15                    0.942347       0.501912          0.957030  \n",
       "16                    0.944435       0.501573          0.955404  \n",
       "17                    0.937674       0.500833          0.929719  \n",
       "18                    0.930019       0.501700          0.933854  \n",
       "19                    0.943523       0.501147          0.954399  \n",
       "20                    0.953873       0.500640          0.912794  \n",
       "21                    0.935031       0.500633          0.881875  \n",
       "22                    0.937322       0.501198          0.931966  \n",
       "23                    0.945781       0.501639          0.940470  \n",
       "24                    0.917307       0.500885          0.917530  \n",
       "25                    0.954858       0.502817          0.959759  \n",
       "26                    0.919254       0.502659          0.956082  \n",
       "27                    0.941280       0.501712          0.936901  \n",
       "28                    0.944297       0.502455          0.959178  \n",
       "29                    0.955413       0.501061          0.941061  \n",
       "...                        ...            ...               ...  \n",
       "16353                 0.972099       0.502810          0.940006  \n",
       "16354                 0.973813       0.524724          0.958083  \n",
       "16355                 0.952112       0.502595          0.960338  \n",
       "16356                 0.940776       0.501310          0.940917  \n",
       "16357                 0.942296       0.501388          0.958088  \n",
       "16358                 0.954083       0.501190          0.952499  \n",
       "16359                 0.930311       0.500884          0.947086  \n",
       "16360                 0.938621       0.501409          0.943213  \n",
       "16361                 0.937988       0.501146          0.931656  \n",
       "16362                 0.921131       0.500972          0.922317  \n",
       "16363                 0.922129       0.520682          0.977681  \n",
       "16364                 0.917509       0.501697          0.940016  \n",
       "16365                 0.956198       0.502489          0.958775  \n",
       "16366                 0.930459       0.501824          0.932921  \n",
       "16367                 0.953686       0.501996          0.953966  \n",
       "16368                 0.959734       0.501151          0.933029  \n",
       "16369                 0.962759       0.501341          0.937401  \n",
       "16370                 0.925530       0.501913          0.952027  \n",
       "16371                 0.954414       0.501333          0.814025  \n",
       "16372                 0.949549       0.501102          0.932882  \n",
       "16373                 0.940694       0.501076          0.679697  \n",
       "16374                 0.949872       0.502044          0.954603  \n",
       "16375                 0.956850       0.501307          0.955261  \n",
       "16376                 0.941861       0.501457          0.955670  \n",
       "16377                 0.940081       0.501314          0.931658  \n",
       "16378                 0.950846       0.502340          0.957395  \n",
       "16379                 0.924664       0.501063          0.928478  \n",
       "16380                 0.936418       0.501865          0.936170  \n",
       "16381                 0.959669       0.501615          0.955774  \n",
       "16382                 0.960789       0.501182          0.959291  \n",
       "\n",
       "[16383 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
