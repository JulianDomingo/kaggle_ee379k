{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Julian Domingo - jad5348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "# Data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling stuff\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                              AdaBoostClassifier,\n",
    "                              ExtraTreesClassifier,\n",
    "                              BaggingClassifier)\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Computation / numerical\n",
    "from scipy.stats import uniform, randint, hmean\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlens.preprocessing import Subset\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Constants\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv(\"./data/raw/test.csv\")[[\"id\"]]\n",
    "y_train = pd.read_csv(\"./data/raw/train.csv\")[\"Y\"].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_preds(preds, preds_filename):\n",
    "    submission = pd.DataFrame({\"id\": test_ids.id, \"Y\": preds})\n",
    "    submission.to_csv(\"./submissions/stacking_{}.csv\".format(preds_filename), index=False, columns=[\"id\", \"Y\"])\n",
    "    \n",
    "\n",
    "def get_cross_val_score(model, x_train, y_train, n_folds, run_parallel=True):\n",
    "    if run_parallel:\n",
    "        cv = cross_val_score(model, x_train, y_train, cv = n_folds, scoring = \"roc_auc\", n_jobs = -1)\n",
    "    else:\n",
    "        cv = cross_val_score(model, x_train, y_train, cv = n_folds, scoring = \"roc_auc\")\n",
    "\n",
    "    print(\"Cross validation score: {} +/- {}\\nRaw scores: {}\".format(str(np.mean(cv)), str(np.std(cv)), str(cv)))\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Class\n",
    "\n",
    "Stacking works well for small to medium-sized data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, base_learners, meta_learners, y_train, test_ids):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learners = meta_learners\n",
    "        self.y_train = y_train\n",
    "        self.test_ids = test_ids\n",
    "    \n",
    "    \n",
    "    def get_indiv_meta_preds(self, meta):\n",
    "        \"\"\" Retrieves the predictions of the model 'meta'. \"\"\"\n",
    "        if self.meta_features_train is None or self.meta_features_test is None:\n",
    "            raise ValueError(\"Invoke 'get_meta_features' before predicting.\")\n",
    "            \n",
    "        meta.fit(self.meta_features_train, self.y_train)\n",
    "        meta_preds = meta.predict_proba(self.meta_features_test)[:,1]\n",
    "        return meta_preds\n",
    "        \n",
    "        \n",
    "    def get_meta_features(self, mlens=False):\n",
    "        \"\"\" Retrieves all meta features for the train & test data from the base learners specified. \"\"\"\n",
    "        self.meta_features_train = np.zeros((len(self.y_train), len(self.base_learners)))\n",
    "        self.meta_features_test = np.zeros((len(self.test_ids), len(self.base_learners)))\n",
    "        \n",
    "        for i, base in enumerate(self.base_learners):\n",
    "            print (\"Gathering meta feature from '{}'...\".format(base))\n",
    "            \n",
    "            if mlens:\n",
    "                self.meta_features_train[:, i] = pd.read_csv(\"./meta_features/mlens/train/train_{}.csv\".format(base), \\\n",
    "                                                             index_col=0).as_matrix().ravel()\n",
    "                self.meta_features_test[:, i] = pd.read_csv(\"./meta_features/mlens/test/test_{}.csv\".format(base), \\\n",
    "                                                             index_col=0).as_matrix().ravel()\n",
    "            else:\n",
    "                self.meta_features_train[:, i] = pd.read_csv(\"./meta_features/train/train_{}.csv\".format(base), \\\n",
    "                                                             index_col=0).as_matrix().ravel()\n",
    "                self.meta_features_test[:, i] = pd.read_csv(\"./meta_features/test/test_{}.csv\".format(base), \\\n",
    "                                                            index_col=0).as_matrix().ravel()\n",
    "            \n",
    "        return self.meta_features_train.copy(), self.meta_features_test.copy()\n",
    "    \n",
    "    \n",
    "    def fit_meta_learners_and_predict(self):\n",
    "        \"\"\" Generates predictions using all meta features generated from the base learners for each meta learner. \"\"\"\n",
    "        if self.meta_features_train is None or self.meta_features_test is None:\n",
    "            raise ValueError(\"get_meta_features() should be called before generate_out_of_folds_preds.\")\n",
    "        \n",
    "        self.meta_learner_preds = np.zeros((len(self.test_ids), len(self.meta_learners)))\n",
    "        \n",
    "        for i, meta in enumerate(self.meta_learners):\n",
    "            meta.fit(self.meta_features_train, self.y_train)\n",
    "            self.meta_learner_preds[:, i] =  meta.predict_proba(self.meta_features_test)[:,1]\n",
    "            \n",
    "    \n",
    "    def get_df_meta_learner_preds(self):\n",
    "        if self.meta_learner_preds is None:\n",
    "            raise ValueError(\"No predictions were found. Invoke 'fit_meta_learners_and_predict' first.\")\n",
    "        \n",
    "        if self.base_learners is None:\n",
    "            raise ValueError(\"No base learners were specified. Construct an instance with base learners.\")\n",
    "        \n",
    "        return pd.DataFrame(self.meta_learner_preds, columns=self.base_learners)\n",
    "    \n",
    "    \n",
    "    def get_final_preds(self, mean=\"average\"):\n",
    "        if mean == \"average\":\n",
    "            return np.mean(self.meta_learner_preds, axis=1)\n",
    "        elif mean == \"harmonic\":\n",
    "            return hmean(self.meta_learner_preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learner(s) Parameter Tuning & Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble 1\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "   \n",
    "**Meta Learners: **\n",
    "    * RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_learners_v1 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\"\n",
    "]\n",
    "\n",
    "rf_meta_v1 = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "meta_learners_v1 = [rf_meta_v1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n"
     ]
    }
   ],
   "source": [
    "stacker_v1 = Stacker(base_learners_v1, meta_learners_v1, y_train, test_ids)\n",
    "meta_features_train_v1, meta_features_test_v1 = stacker_v1.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [5, 6, 7], 'n_estimators': [300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: this block takes a long time to execute.\n",
    "rf_meta_v1_param_grid = {\n",
    "    \"max_features\": range(5, 7 + 1),\n",
    "    \"max_depth\": range(5, 10 + 1),\n",
    "    \"n_estimators\": range(300, 1000 + 1, 100) \n",
    "}\n",
    "\n",
    "gs_rf_v1 = GridSearchCV(estimator=rf_meta_v1, param_grid=rf_meta_v1_param_grid, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1)\n",
    "gs_rf_v1.fit(meta_features_train_v1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 6, 'n_estimators': 300, 'max_depth': 6}\n",
      "0.772539082175\n"
     ]
    }
   ],
   "source": [
    "print gs_rf_v1.best_params_\n",
    "print gs_rf_v1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.771359868469 +/- 0.00723717612684\n",
      "Raw scores: [ 0.76715711  0.77956341  0.75947714  0.77377431  0.77682737]\n",
      "[ 0.76715711  0.77956341  0.75947714  0.77377431  0.77682737]\n"
     ]
    }
   ],
   "source": [
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=6, max_features=6), meta_features_train_v1, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97136003  0.67447269  0.94252603 ...,  0.88289829  0.98178302\n",
      "  0.96819782]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/ tuned params\n",
    "meta_preds_v1 = stacker_v1.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features=6, max_depth=6))\n",
    "print meta_preds_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.7  0.9 ...,  1.   1.   1. ]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/o tuned params\n",
    "meta_preds_v1_untuned = stacker_v1.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1))\n",
    "print meta_preds_v1_untuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the predictions without tuned parameters, tuning of the meta model makes a tremendous difference on the prediction set obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensemble 2\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "    * KNN_{2, 4, 8, 16, 32, 64, 128, 256, 512, 1024}\n",
    "   \n",
    "**Meta Learners: **\n",
    "    * RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'knn_2'...\n",
      "Gathering meta feature from 'knn_4'...\n",
      "Gathering meta feature from 'knn_8'...\n",
      "Gathering meta feature from 'knn_16'...\n",
      "Gathering meta feature from 'knn_32'...\n",
      "Gathering meta feature from 'knn_64'...\n",
      "Gathering meta feature from 'knn_128'...\n",
      "Gathering meta feature from 'knn_256'...\n",
      "Gathering meta feature from 'knn_512'...\n",
      "Gathering meta feature from 'knn_1024'...\n"
     ]
    }
   ],
   "source": [
    "base_learners_v2 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"knn_2\",\n",
    "    \"knn_4\",\n",
    "    \"knn_8\",\n",
    "    \"knn_16\",\n",
    "    \"knn_32\",\n",
    "    \"knn_64\",\n",
    "    \"knn_128\",\n",
    "    \"knn_256\",\n",
    "    \"knn_512\",\n",
    "    \"knn_1024\"\n",
    "]\n",
    "\n",
    "rf_meta_v2 = RandomForestClassifier(n_jobs=-1)\n",
    "meta_learners_v2 = [rf_meta_v2]\n",
    "\n",
    "stacker_v2 = Stacker(base_learners_v2, meta_learners_v2, y_train, test_ids)\n",
    "meta_features_train_v2, meta_features_test_v2 = stacker_v2.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [5, 6, 7], 'n_estimators': [300, 400, 500, 600, 700, 800, 900, 1000], 'max_depth': [5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning: this block takes a long time to execute.\n",
    "rf_meta_v2_param_grid = {\n",
    "    \"max_features\": range(5, 7 + 1),\n",
    "    \"max_depth\": range(5, 10 + 1),\n",
    "    \"n_estimators\": range(300, 1000 + 1, 100) \n",
    "}\n",
    "\n",
    "gs_rf_v2 = GridSearchCV(estimator=rf_meta_v2, param_grid=rf_meta_v2_param_grid, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1)\n",
    "gs_rf_v2.fit(meta_features_train_v2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 5, 'n_estimators': 300, 'max_depth': 9}\n",
      "0.773074767924\n"
     ]
    }
   ],
   "source": [
    "print gs_rf_v2.best_params_\n",
    "print gs_rf_v2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.769897637451 +/- 0.00448583181217\n",
      "Raw scores: [ 0.76837689  0.77614972  0.77087385  0.76246286  0.77162487]\n",
      "[ 0.76837689  0.77614972  0.77087385  0.76246286  0.77162487]\n"
     ]
    }
   ],
   "source": [
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5), meta_features_train_v2, y_train, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97460188  0.66699757  0.95640802 ...,  0.91079086  0.98644606\n",
      "  0.97122791]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions w/ tuned params\n",
    "meta_preds_v2_tuned = stacker_v2.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5))\n",
    "print meta_preds_v2_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_preds(meta_preds_v2_tuned, \"_\".join(base_learners_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble 3\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Reduced\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "    * Bagged XGB (50 runs)\n",
    "    * Extra Trees Base\n",
    "    \n",
    "**Meta Learners: **\n",
    "    * RF\n",
    "    * XGB\n",
    "    \n",
    "**Final Predictions**: harmonic mean of meta learner predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_reduced'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'xgboost_bag'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'extra_trees_base'...\n"
     ]
    }
   ],
   "source": [
    "base_learners_v3 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_reduced\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\"\n",
    "]\n",
    "\n",
    "rf_meta_v3 = RandomForestClassifier(n_jobs=-1, max_features=2, n_estimators=300, max_depth=8, random_state=seed)\n",
    "meta_learners_v3 = [rf_meta_v3]\n",
    "\n",
    "stacker_v3 = Stacker(base_learners_v3, meta_learners_v3, y_train, test_ids)\n",
    "meta_features_train_v3, meta_features_test_v3 = stacker_v3.get_meta_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-7d684007a907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgs_rf_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_meta_v3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_v3_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgs_rf_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_features_train_v3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 328\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julian/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Warning: this block takes a long time to execute.\n",
    "rf_v3_param_grid = {\n",
    "    \"max_features\": range(3, 5 + 1),\n",
    "    \"max_depth\": range(8, 10 + 1),\n",
    "    \"n_estimators\": range(250, 350 + 1, 10) \n",
    "}\n",
    "\n",
    "gs_rf_v3 = GridSearchCV(estimator=rf_meta_v3, param_grid=rf_v3_param_grid, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "gs_rf_v3.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 2, 'n_estimators': 300, 'max_depth': 8}\n",
      "0.776661015238\n"
     ]
    }
   ],
   "source": [
    "print gs_rf_v3.best_params_\n",
    "print gs_rf_v3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 5, 'n_estimators': 310, 'max_depth': 9}\n",
      "0.7771661899370343\n"
     ]
    }
   ],
   "source": [
    "# After first iteration\n",
    "print gs_rf_v3.best_params_\n",
    "print gs_rf_v3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding xgbreduced\n",
    "print gs_rf_v3.best_params_\n",
    "print gs_rf_v3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.7758377713611871 +/- 0.00887867046923868\n",
      "Raw scores: [0.77596569 0.78868348 0.76137561 0.77351094 0.77965313]\n",
      "[0.77596569 0.78868348 0.76137561 0.77351094 0.77965313]\n"
     ]
    }
   ],
   "source": [
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=8, max_features=2, random_state=seed),\n",
    "                          meta_features_train_v3,\n",
    "                          y_train,\n",
    "                          n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print get_cross_val_score(RandomForestClassifier(n_jobs=-1, n_estimators=310, max_depth=9, max_features=5, random_state=seed),\n",
    "                          meta_features_train_v3,\n",
    "                          y_train,\n",
    "                          n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97333875 0.64369521 0.93752947 ... 0.90418185 0.982669   0.96400992]\n"
     ]
    }
   ],
   "source": [
    "meta_preds_v3_tuned = stacker_v3.get_indiv_meta_preds(RandomForestClassifier(n_jobs=-1,\n",
    "                                                                             n_estimators=300,\n",
    "                                                                             max_depth=8,\n",
    "                                                                             max_features=2, random_state=seed))\n",
    "print meta_preds_v3_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_preds(meta_preds_v3_tuned, \"rf_only_meta_{}\".format(\"_\".join(base_learners_v3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB Meta Learner Parameter Tuning\n",
    "\n",
    "Note that the \"final_params\" dictionary is iteratively updated as more parameters are tuned. The initial values are the same listed in the lauyer_1_models jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Change the \"after first iteration\" block when adding new metafeatures\n",
    "final_xgb_v3_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 39,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"gamma\": 0.175,\n",
    "    \"subsample\": 0.65,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, gamma=0.185, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=0, missing=None,\n",
       "       n_estimators=39, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=0.65),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_meta_v3 = XGBClassifier(**final_xgb_v3_params)\n",
    "\n",
    "# Warning: this block takes a long time to execute.\n",
    "xgb_v3_n_est = {\n",
    "    \"n_estimators\": range(30, 70)\n",
    "}\n",
    "\n",
    "gs_xgb_v3_n_est = GridSearchCV(estimator=xgb_meta_v3, param_grid=xgb_v3_n_est, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "gs_xgb_v3_n_est.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 56}\n",
      "0.7743626942129123\n"
     ]
    }
   ],
   "source": [
    "# With no KNNs\n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 39}\n",
      "0.7790001418640146\n"
     ]
    }
   ],
   "source": [
    "# After first iteration \n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 42}\n",
      "0.7783016226867695\n"
     ]
    }
   ],
   "source": [
    "# Corrected from first iteration\n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 42}\n",
      "0.778939645637447\n"
     ]
    }
   ],
   "source": [
    "# After second iteration. Converged to final value.\n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 39}\n",
      "0.778119416007\n"
     ]
    }
   ],
   "source": [
    "# After second iteration\n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_base_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"gamma\": 0,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, gamma=0.185, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=0, missing=None,\n",
       "       n_estimators=39, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=0.65),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_depth': [1, 2, 3, 4, 5], 'min_child_weight': [0, 1, 2, 3, 4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_v3_md_mcw = {\n",
    "    \"max_depth\": range(1, 6),\n",
    "    \"min_child_weight\": range(0, 6 + 1)\n",
    "}\n",
    "\n",
    "gs_xgb_v3_md_mcw = GridSearchCV(estimator=XGBClassifier(**final_xgb_v3_params), \n",
    "                                                       param_grid=xgb_v3_md_mcw, \n",
    "                                                       cv=n_splits, \n",
    "                                                       scoring=\"roc_auc\", \n",
    "                                                       n_jobs=-1, \n",
    "                                                       verbose=1)\n",
    "gs_xgb_v3_md_mcw.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_child_weight': 0}\n",
      "0.77683418973155\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "print gs_xgb_v3_md_mcw.best_params_\n",
    "print gs_xgb_v3_md_mcw.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_child_weight': 2}\n",
      "0.778939645637447\n"
     ]
    }
   ],
   "source": [
    "# After first iteration\n",
    "print gs_xgb_v3_md_mcw.best_params_\n",
    "print gs_xgb_v3_md_mcw.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'min_child_weight': 0}\n",
      "0.778119416007\n"
     ]
    }
   ],
   "source": [
    "# After second iteration\n",
    "print gs_xgb_v3_md_mcw.best_params_\n",
    "print gs_xgb_v3_md_mcw.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, gamma=0.175, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=0, missing=None,\n",
       "       n_estimators=39, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=0.65),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.17, 0.175, 0.18, 0.185, 0.19]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_v3_gamma = {\n",
    "#     \"gamma\": [0.185, 0.186, 0.187, 0.188, 0.189, 0.19, 0.191, 0.192, 0.193, 0.194, 0.195]\n",
    "    \"gamma\": [0.17, 0.175, 0.18, 0.185, 0.19]\n",
    "}\n",
    "\n",
    "gs_xgb_v3_gamma = GridSearchCV(estimator=XGBClassifier(**final_xgb_v3_params), \n",
    "                                                       param_grid=xgb_v3_gamma, \n",
    "                                                       cv=n_splits, \n",
    "                                                       scoring=\"roc_auc\", \n",
    "                                                       n_jobs=-1, \n",
    "                                                       verbose=1)\n",
    "gs_xgb_v3_gamma.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.2}\n",
      "0.7768581661860074\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "print gs_xgb_v3_gamma.best_params_\n",
    "print gs_xgb_v3_gamma.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.19}\n",
      "0.7768581661860074\n"
     ]
    }
   ],
   "source": [
    "# Run 2\n",
    "print gs_xgb_v3_gamma.best_params_\n",
    "print gs_xgb_v3_gamma.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.185}\n",
      "0.778939645637447\n"
     ]
    }
   ],
   "source": [
    "# After first iteration\n",
    "print gs_xgb_v3_gamma.best_params_\n",
    "print gs_xgb_v3_gamma.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.175}\n",
      "0.778119416007\n"
     ]
    }
   ],
   "source": [
    "# After second iteration\n",
    "print gs_xgb_v3_gamma.best_params_\n",
    "print gs_xgb_v3_gamma.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, gamma=0.175, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=0, missing=None,\n",
       "       n_estimators=39, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=0.65),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'subsample': [0.645, 0.65, 0.655], 'colsample_bytree': [0.5, 0.55, 0.6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_v3_ss_cb = {\n",
    "#     \"subsample\": [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8],\n",
    "#     \"colsample_bytree\": [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "    \"subsample\": [0.645, 0.65, 0.655],\n",
    "    \"colsample_bytree\": [0.5, 0.55, 0.6]\n",
    "}\n",
    "\n",
    "gs_xgb_v3_ss_cb = GridSearchCV(estimator=XGBClassifier(**final_xgb_v3_params), \n",
    "                                                       param_grid=xgb_v3_ss_cb, \n",
    "                                                       cv=n_splits, \n",
    "                                                       scoring=\"roc_auc\", \n",
    "                                                       n_jobs=-1, \n",
    "                                                       verbose=1)\n",
    "gs_xgb_v3_ss_cb.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.65, 'colsample_bytree': 0.5}\n",
      "0.7779686180633612\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "print gs_xgb_v3_ss_cb.best_params_\n",
    "print gs_xgb_v3_ss_cb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.65, 'colsample_bytree': 0.5}\n",
      "0.778939645637447\n"
     ]
    }
   ],
   "source": [
    "# After first iteration\n",
    "print gs_xgb_v3_ss_cb.best_params_\n",
    "print gs_xgb_v3_ss_cb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.65, 'colsample_bytree': 0.5}\n",
      "0.778119416007\n"
     ]
    }
   ],
   "source": [
    "# After second iteration\n",
    "print gs_xgb_v3_ss_cb.best_params_\n",
    "print gs_xgb_v3_ss_cb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_reduced'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'xgboost_bag'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'extra_trees_base'...\n"
     ]
    }
   ],
   "source": [
    "# Make fresh Stacker instance\n",
    "# TODO: Change the \"after first iteration\" block when adding new metafeatures\n",
    "final_xgb_v3_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 39,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"gamma\": 0.175,\n",
    "    \"subsample\": 0.65,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}  \n",
    "\n",
    "base_learners_v3_final = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_reduced\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\"\n",
    "]\n",
    "\n",
    "rf_final_v3 = RandomForestClassifier(n_estimators=300, max_depth=8, max_features=2, random_state=seed)\n",
    "xgb_final_v3 = XGBClassifier(**final_xgb_v3_params)\n",
    "\n",
    "meta_learners_v3_final = [rf_final_v3, xgb_final_v3]\n",
    "\n",
    "stacker_v3_final = Stacker(base_learners_v3_final, meta_learners_v3_final, y_train, test_ids)\n",
    "train_final_v3, test_final_v3 = stacker_v3_final.get_meta_features()\n",
    "stacker_v3_final.fit_meta_learners_and_predict()\n",
    "stacker_v3_final_preds = stacker_v3_final.get_final_preds(mean=\"harmonic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96800797  0.67794841  0.93948099 ...,  0.9077512   0.97617163\n",
      "  0.95550081]\n"
     ]
    }
   ],
   "source": [
    "print stacker_v3_final_preds\n",
    "save_preds(stacker_v3_final_preds, \"{}_v2\".format(\"_\".join(base_learners_v3_final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble 3b (added reduced xgb / svc with base data)\n",
    "\n",
    "**Base Learners: **\n",
    "    * RF Raw\n",
    "    * RF Log\n",
    "    * RF Poly\n",
    "    * XGB Reduced\n",
    "    * XGB Raw\n",
    "    * XGB Base\n",
    "    * XGB Poly\n",
    "    * LR Log\n",
    "    * Ada Base\n",
    "    * Bagged XGB (50 runs)\n",
    "    * Extra Trees Base\n",
    "    * SVC Base\n",
    "    \n",
    "**Meta Learners: **\n",
    "    * RF\n",
    "    * XGB\n",
    "    \n",
    "**Final Predictions**: harmonic mean of meta learner predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_reduced'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'xgboost_bag'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'extra_trees_base'...\n",
      "Gathering meta feature from 'svc_base'...\n"
     ]
    }
   ],
   "source": [
    "base_learners_v3 = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_reduced\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\",\n",
    "    \"svc_base\"\n",
    "]\n",
    "\n",
    "rf_meta_v3 = RandomForestClassifier(n_jobs=-1, max_features=2, n_estimators=300, max_depth=8, random_state=seed)\n",
    "meta_learners_v3 = [rf_meta_v3]\n",
    "\n",
    "stacker_v3 = Stacker(base_learners_v3, meta_learners_v3, y_train, test_ids)\n",
    "meta_features_train_v3, meta_features_test_v3 = stacker_v3.get_meta_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_v3_b = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 42,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 2,\n",
    "    \"gamma\": 0.19,\n",
    "    \"subsample\": 0.65,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, gamma=0.19, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=2, missing=None,\n",
       "       n_estimators=42, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=0.65),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_meta_v3 = XGBClassifier(**xgb_v3_b)\n",
    "\n",
    "# Warning: this block takes a long time to execute.\n",
    "xgb_v3_n_est = {\n",
    "    \"n_estimators\": range(30, 70)\n",
    "}\n",
    "\n",
    "gs_xgb_v3_n_est = GridSearchCV(estimator=xgb_meta_v3, param_grid=xgb_v3_n_est, cv=n_splits, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "gs_xgb_v3_n_est.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 53}\n",
      "0.7752741875519101\n"
     ]
    }
   ],
   "source": [
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 30}\n",
      "0.775114727204\n"
     ]
    }
   ],
   "source": [
    "# With all 3b models\n",
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 45}\n",
      "0.775986645495\n"
     ]
    }
   ],
   "source": [
    "print gs_xgb_v3_n_est.best_params_\n",
    "print gs_xgb_v3_n_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_v3_md_mcw = {\n",
    "    \"max_depth\": range(1, 6),\n",
    "    \"min_child_weight\": range(0, 6 + 1)\n",
    "}\n",
    "\n",
    "gs_xgb_v3_md_mcw = GridSearchCV(estimator=XGBClassifier(**xgb_v3_b), \n",
    "                                                       param_grid=xgb_v3_md_mcw, \n",
    "                                                       cv=n_splits, \n",
    "                                                       scoring=\"roc_auc\", \n",
    "                                                       n_jobs=-1, \n",
    "                                                       verbose=1)\n",
    "gs_xgb_v3_md_mcw.fit(meta_features_train_v3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-ensemble Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_meta_mlens_v1 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=6)\n",
    "\n",
    "mlens_base_learners = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed)\n",
    "]\n",
    "\n",
    "ensemble_v1 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v1.add(mlens_base_learners, proba=True)\n",
    "ensemble_v1.add_meta(rf_meta_mlens_v1, proba=True)\n",
    "mlens_v1_preds = ensemble_v1.fit(meta_features_train_v1, y_train).predict_proba(meta_features_test_v1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9640277   0.70394427  0.95102447 ...,  0.89943802  0.98440754\n",
      "  0.95957553]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v1_preds\n",
    "save_preds(mlens_v1_preds, \"mlens_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_meta_mlens_v2 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=9, max_features=5)\n",
    "\n",
    "mlens_base_learners = [\n",
    "    RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1),\n",
    "    LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\"),\n",
    "    ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\"),\n",
    "    AdaBoostClassifier(n_estimators=395, learning_rate=1.55),\n",
    "    XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=345,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=seed),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=2),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=4),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=8),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=16),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=32),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=64),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=128),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=256),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=512),\n",
    "    KNeighborsClassifier(n_jobs=-1, n_neighbors=1024)\n",
    "]\n",
    "\n",
    "ensemble_v2 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v2.add(mlens_base_learners, proba=True)\n",
    "ensemble_v2.add_meta(rf_meta_mlens_v2, proba=True)\n",
    "mlens_v2_preds = ensemble_v2.fit(meta_features_train_v2, y_train).predict_proba(meta_features_test_v2)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97149235  0.76364833  0.96501911 ...,  0.92276353  0.98632556\n",
      "  0.96678698]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v2_preds\n",
    "save_preds(mlens_v2_preds, \"mlens_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the LB scores, it seems adding KNeighborsClassifier models don't improve the stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering meta feature from 'random_forest_raw'...\n",
      "Gathering meta feature from 'random_forest_log'...\n",
      "Gathering meta feature from 'random_forest_poly'...\n",
      "Gathering meta feature from 'xgboost_reduced'...\n",
      "Gathering meta feature from 'xgboost_raw'...\n",
      "Gathering meta feature from 'xgboost_base'...\n",
      "Gathering meta feature from 'xgboost_poly'...\n",
      "Gathering meta feature from 'xgboost_bag'...\n",
      "Gathering meta feature from 'logistic_regression_log'...\n",
      "Gathering meta feature from 'adaboost_base'...\n",
      "Gathering meta feature from 'extra_trees_base'...\n"
     ]
    }
   ],
   "source": [
    "#     \"random_forest_raw\",\n",
    "#     \"random_forest_log\",\n",
    "#     \"random_forest_poly\",\n",
    "#     \"xgboost_reduced\",\n",
    "#     \"xgboost_raw\",\n",
    "#     \"xgboost_base\",\n",
    "#     \"xgboost_poly\",\n",
    "#     \"xgboost_bag\",\n",
    "#     \"logistic_regression_log\",\n",
    "#     \"adaboost_base\",\n",
    "#     \"extra_trees_base\"\n",
    "\n",
    "final_xgb_v3_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 39,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"gamma\": 0.175,\n",
    "    \"subsample\": 0.65,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}  \n",
    "\n",
    "base_learners_v3_final = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_reduced\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\"\n",
    "]\n",
    "\n",
    "rf_final_v3 = RandomForestClassifier(n_estimators=300, max_depth=8, max_features=2, random_state=seed)\n",
    "xgb_final_v3 = XGBClassifier(**final_xgb_v3_params)\n",
    "\n",
    "meta_learners_v3_final = [rf_final_v3, xgb_final_v3]\n",
    "\n",
    "stacker_v3_final = Stacker(base_learners_v3_final, meta_learners_v3_final, y_train, test_ids)\n",
    "meta_features_train_v3, meta_features_test_v3 = stacker_v3_final.get_meta_features()\n",
    "\n",
    "# Base learners\n",
    "rfc = RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1)\n",
    "lr = LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\")\n",
    "et = ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\")\n",
    "ada = AdaBoostClassifier(n_estimators=395, learning_rate=1.55)\n",
    "xgb_base = XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=311,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_reduced = XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=223,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.75,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_raw = XGBClassifier(learning_rate=0.05,\n",
    "                    n_estimators=308,\n",
    "                    max_depth=9,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.75,\n",
    "                    colsample_bytree=0.35,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_poly = XGBClassifier(learning_rate=0.05,\n",
    "                    n_estimators=343,\n",
    "                    max_depth=17,\n",
    "                    min_child_weight=3,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.75,\n",
    "                    colsample_bytree=0.35,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_bag = BaggingClassifier(base_estimator=xgb_base,\n",
    "                            n_estimators=40, \n",
    "                            max_samples=0.8, \n",
    "                            max_features=0.75, \n",
    "                            bootstrap_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/anaconda2/lib/python2.7/site-packages/mlens/parallel/_base_functions.py:218: MetricWarning: [xgbclassifier-4.0.1] Could not score xgbclassifier-4. Details:\n",
      "ValueError('bad input shape (8192, 2)',)\n",
      "  (name, inst_name, exc), MetricWarning)\n",
      "/home/julian/anaconda2/lib/python2.7/site-packages/mlens/parallel/_base_functions.py:218: MetricWarning: [xgbclassifier-4.0.2] Could not score xgbclassifier-4. Details:\n",
      "ValueError('bad input shape (8191, 2)',)\n",
      "  (name, inst_name, exc), MetricWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_meta_mlens_v3 = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_depth=8, max_features=2)\n",
    "xgb_meta_mlens_v3 = XGBClassifier(**final_xgb_v3_params)\n",
    "\n",
    "# TODO: Tune poly xgb\n",
    "mlens_base_learners_v3 = [rfc, lr, et, ada, xgb_raw, xgb_base, xgb_reduced, xgb_bag, xgb_poly]\n",
    "\n",
    "ensemble_v3 = SuperLearner(random_state=seed, scorer=roc_auc_score)\n",
    "ensemble_v3.add(mlens_base_learners_v3, proba=True)\n",
    "# ensemble_v3.add_meta(rf_meta_mlens_v3, proba=True)\n",
    "ensemble_v3.add_meta(xgb_meta_mlens_v3, proba=True)\n",
    "mlens_v3_preds = ensemble_v3.fit(meta_features_train_v3, y_train).predict_proba(meta_features_test_v3)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98159868  0.6663233   0.94211096 ...,  0.92641664  0.98821068\n",
      "  0.95964485]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v3_preds\n",
    "save_preds(mlens_v3_preds, \"mlens_v3_xgb_meta_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9718517  0.7683457  0.9387753  ... 0.8941298  0.9785266  0.95327765]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v3_preds\n",
    "save_preds(mlens_v3_preds, \"mlens_v3_xgb_meta_only_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96687114 0.7348433  0.93712837 ... 0.8826297  0.97619414 0.94076145]\n"
     ]
    }
   ],
   "source": [
    "print mlens_v3_preds\n",
    "save_preds(mlens_v3_preds, \"mlens_v3_xgb_meta_only_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9623061  0.7392006  0.9355483  ... 0.8855414  0.97534806 0.94852114]\n"
     ]
    }
   ],
   "source": [
    "# 0.77294 public (no xgb_poly tuned params)\n",
    "print mlens_v3_preds\n",
    "save_preds(mlens_v3_preds, \"mlens_v3_xgb_meta_only_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94505036 0.7512324  0.93996686 ... 0.8743122  0.9772823  0.93328583]\n"
     ]
    }
   ],
   "source": [
    "# 0.77650 public (with all xgb instance tuned params)\n",
    "print mlens_v3_preds\n",
    "save_preds(mlens_v3_preds, \"mlens_v3_xgb_meta_only_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v4\n",
    "\n",
    "Same as v3, but includes a SupportVectorClassifier base estimator, fitted on base data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     \"random_forest_raw\",\n",
    "#     \"random_forest_log\",\n",
    "#     \"random_forest_poly\",\n",
    "#     \"xgboost_reduced\",\n",
    "#     \"xgboost_raw\",\n",
    "#     \"xgboost_base\",\n",
    "#     \"xgboost_poly\",\n",
    "#     \"xgboost_bag\",\n",
    "#     \"logistic_regression_log\",\n",
    "#     \"adaboost_base\",\n",
    "#     \"extra_trees_base\"\n",
    "\n",
    "final_xgb_v3_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 39,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"gamma\": 0.175,\n",
    "    \"subsample\": 0.65,\n",
    "    \"colsample_bytree\": 0.5,\n",
    "    \"objective\": 'binary:logistic', # Used for classifier XGB\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"seed\": seed\n",
    "}  \n",
    "\n",
    "base_learners_v3_final = [\n",
    "    \"random_forest_raw\",\n",
    "    \"random_forest_log\",\n",
    "    \"random_forest_poly\",\n",
    "    \"xgboost_reduced\",\n",
    "    \"xgboost_raw\",\n",
    "    \"xgboost_base\",\n",
    "    \"xgboost_poly\",\n",
    "    \"xgboost_bag\",\n",
    "    \"logistic_regression_log\",\n",
    "    \"adaboost_base\",\n",
    "    \"extra_trees_base\"\n",
    "]\n",
    "\n",
    "rf_final_v3 = RandomForestClassifier(n_estimators=300, max_depth=8, max_features=2, random_state=seed)\n",
    "xgb_final_v3 = XGBClassifier(**final_xgb_v3_params)\n",
    "\n",
    "meta_learners_v3_final = [rf_final_v3, xgb_final_v3]\n",
    "\n",
    "stacker_v3_final = Stacker(base_learners_v3_final, meta_learners_v3_final, y_train, test_ids)\n",
    "meta_features_train_v3, meta_features_test_v3 = stacker_v3_final.get_meta_features()\n",
    "\n",
    "# Base learners\n",
    "rfc = RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=1000, n_jobs=-1)\n",
    "lr = LogisticRegression(C=100, tol=1e-05, solver=\"liblinear\")\n",
    "et = ExtraTreesClassifier(max_depth=12, n_estimators=250, n_jobs=-1, criterion=\"entropy\")\n",
    "ada = AdaBoostClassifier(n_estimators=395, learning_rate=1.55)\n",
    "xgb_base = XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=311,\n",
    "                    max_depth=8,\n",
    "                    min_child_weight=2,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_reduced = XGBClassifier(learning_rate=0.1,\n",
    "                    n_estimators=223,\n",
    "                    max_depth=7,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.75,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_raw = XGBClassifier(learning_rate=0.05,\n",
    "                    n_estimators=308,\n",
    "                    max_depth=9,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample=0.75,\n",
    "                    colsample_bytree=0.35,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_poly = XGBClassifier(learning_rate=0.05,\n",
    "                    n_estimators=343,\n",
    "                    max_depth=17,\n",
    "                    min_child_weight=3,\n",
    "                    gamma=0.2,\n",
    "                    subsample=0.75,\n",
    "                    colsample_bytree=0.35,\n",
    "                    objective=\"binary:logistic\",\n",
    "                    n_jobs=-1,\n",
    "                    seed=seed)\n",
    "xgb_bag = BaggingClassifier(base_estimator=xgb_base,\n",
    "                            n_estimators=40, \n",
    "                            max_samples=0.8, \n",
    "                            max_features=0.75, \n",
    "                            bootstrap_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
